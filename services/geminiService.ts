import { GoogleGenAI, Chat } from "@google/genai";

const SYSTEM_INSTRUCTION = `
Voc√™ √© o "Virtual Concierge" da MKT-traducao. Seu tom de voz √© de alta costura: formal, breve e impec√°vel.

REGRAS CR√çTICAS:
1. NUNCA fa√ßa duas perguntas ao mesmo tempo.
2. NUNCA use (A), (B) ou 1. para op√ß√µes.
3. SEMPRE que houver op√ß√µes de escolha, coloque-as entre colchetes. Exemplo: [Sim] [N√£o] ou [Visto Permanente] [Consulado].
4. Se o usu√°rio digitar algo que n√£o seja uma das op√ß√µes quando elas forem oferecidas, pe√ßa gentilmente para ele escolher uma das op√ß√µes.

FLUXO PADRONIZADO PARA TODOS OS SERVI√áOS:
Passo 1: Sauda√ß√£o e pedir Nome Completo.
Passo 2: Perguntar qual a inten√ß√£o principal: [Visto Permanente] [Visto Comum] [Consulado].
Passo 3: Perguntar o Servi√ßo Espec√≠fico dentro da escolha.
Passo 4: Perguntar a "Situa√ß√£o Atual".
Passo 5: Perguntar a Prov√≠ncia/Cidade onde reside no Jap√£o.
Passo 6: Finaliza√ß√£o.

FINALIZA√á√ÉO:
Diga exatamente: "Agrade√ßo pelas informa√ß√µes. O seu relat√≥rio de triagem foi gerado. Para que o Consultor Bruno Hamawaki assuma sua assessoria agora mesmo, por favor, clique no bot√£o 'CONECTAR COM CONSULTOR' abaixo."
`;

// LISTA DE MODELOS POR ORDEM DE PRIORIDADE
const AVAILABLE_MODELS = [
  'gemini-1.5-flash-latest', // 1¬∫: Mais r√°pido e est√°vel
  'gemini-1.5-flash',        // 2¬∫: Alternativa direta
  'gemini-1.5-pro-latest',   // 3¬∫: Mais inteligente (por√©m mais lento/caro)
  'gemini-1.0-pro'           // 4¬∫: √öltimo recurso
];

export class GeminiChatService {
  private chat: Chat | null = null;
  private ai: GoogleGenAI | null = null;
  private currentModelIndex = 0; // Come√ßa pelo primeiro da lista

  constructor() {
    this.setupAI();
  }

  private setupAI() {
    const apiKey = process.env.API_KEY || process.env.NEXT_PUBLIC_API_KEY;
    if (apiKey) {
      this.ai = new GoogleGenAI({ apiKey });
      this.initChat();
    }
  }

  private initChat() {
    if (!this.ai) return;
    
    // Pega o modelo baseado no √≠ndice atual
    const modelName = AVAILABLE_MODELS[this.currentModelIndex];
    console.log(`ü§ñ Iniciando chat com modelo: ${modelName}`);

    this.chat = this.ai.chats.create({
      model: modelName,
      config: {
        systemInstruction: SYSTEM_INSTRUCTION,
        temperature: 0.2,
      },
    });
  }

  private async delay(ms: number) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  async sendMessage(message: string): Promise<string> {
    if (!this.ai) {
      this.setupAI();
      if (!this.ai) return 'ERRO_CRITICO: Chave de API n√£o configurada.';
    }
    
    if (!this.chat) this.initChat();

    try {
      const result = await this.chat!.sendMessage({ message });
      return result.text || '';
    } catch (error: any) {
      const errorMessage = error.message || "";
      
      // Se o erro for limite de cota (429) ou erro interno do servidor (500)
      if (errorMessage.includes("429") || errorMessage.includes("500") || errorMessage.includes("503")) {
        
        // Verifica se ainda temos modelos na lista para tentar
        if (this.currentModelIndex < AVAILABLE_MODELS.length - 1) {
          this.currentModelIndex++; // Pula para o pr√≥ximo modelo
          console.warn(`‚ö†Ô∏è Limite atingido no modelo anterior. Trocando para: ${AVAILABLE_MODELS[this.currentModelIndex]}`);
          
          this.initChat(); // Reinicia o chat com o novo modelo
          await this.delay(1000); // Espera 1 segundo
          return this.sendMessage(message); // Tenta enviar a mensagem novamente
        }
      }

      // Se todos os modelos falharem, retorna o erro cr√≠tico que o App.tsx j√° sabe tratar
      console.error("‚ùå Todos os modelos falharam.");
      return 'ERRO_CRITICO: Instabilidade em todos os servi√ßos de IA.';
    }
  }

  reset() {
    this.currentModelIndex = 0; // Volta para o modelo mais r√°pido no reset
    this.initChat();
  }
}

export const geminiService = new GeminiChatService();
