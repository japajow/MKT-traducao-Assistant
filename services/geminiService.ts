import { GoogleGenerativeAI, ChatSession } from "@google/generative-ai";

const SYSTEM_INSTRUCTION = `
Voc√™ √© o "Virtual Concierge" da MKT-traducao. Seu tom de voz √© de alta costura: formal, breve e impec√°vel.

REGRAS CR√çTICAS:
1. NUNCA fa√ßa duas perguntas ao mesmo tempo.
2. SEMPRE coloque as op√ß√µes entre colchetes. Exemplo: [Sim] [N√£o] ou [Visto Permanente] [Consulado].
3. Se o usu√°rio digitar algo fora das op√ß√µes, pe√ßa gentilmente para escolher uma.

FLUXO: Nome Completo -> Inten√ß√£o -> Servi√ßo -> Situa√ß√£o -> Cidade -> Finaliza√ß√£o.

FINALIZA√á√ÉO:
Diga exatamente: "Agrade√ßo pelas informa√ß√µes. O seu relat√≥rio de triagem foi gerado. Para que o Consultor Bruno Hamawaki assuma sua assessoria agora mesmo, por favor, clique no bot√£o 'CONECTAR COM CONSULTOR' abaixo."
`;

const AVAILABLE_MODELS = [
  'gemini-1.5-flash-latest',
  'gemini-1.5-flash',
  'gemini-1.5-pro-latest'
];

export class GeminiChatService {
  private chat: ChatSession | null = null;
  private ai: GoogleGenerativeAI | null = null;
  private currentModelIndex = 0;

  constructor() {
    this.setupAI();
  }

  private setupAI() {
    // Vite usa import.meta.env
    const apiKey = import.meta.env.VITE_API_KEY;
    
    if (apiKey) {
      // CORRE√á√ÉO: GoogleGenerativeAI recebe a string direto, n√£o um objeto
      this.ai = new GoogleGenerativeAI(apiKey);
      this.initChat();
    } else {
      console.error("VITE_API_KEY n√£o encontrada!");
    }
  }

  private initChat() {
    if (!this.ai) return;
    
    const modelName = AVAILABLE_MODELS[this.currentModelIndex];
    console.log(`ü§ñ Iniciando modelo: ${modelName}`);

    try {
      // CORRE√á√ÉO: For√ßando a apiVersion para 'v1' para evitar o erro 404 do v1beta
      const model = this.ai.getGenerativeModel(
        { model: modelName, systemInstruction: SYSTEM_INSTRUCTION },
        { apiVersion: 'v1' }
      );

      this.chat = model.startChat({
        history: [],
        generationConfig: {
          temperature: 0.2,
          maxOutputTokens: 1000,
        },
      });
    } catch (e) {
      console.error("Erro ao iniciar chat:", e);
    }
  }

  async sendMessage(message: string): Promise<string> {
    if (!this.ai || !this.chat) {
      this.setupAI();
      if (!this.ai) return 'ERRO_CRITICO: Chave de API n√£o configurada no Vercel.';
    }

    try {
      const result = await this.chat!.sendMessage(message);
      const response = await result.response;
      return response.text();
    } catch (error: any) {
      const msg = error.message || "";
      console.error("Erro na API:", msg);
      
      // Se for erro de limite ou modelo n√£o encontrado, tenta o pr√≥ximo
      if ((msg.includes("429") || msg.includes("404") || msg.includes("500")) && this.currentModelIndex < AVAILABLE_MODELS.length - 1) {
        this.currentModelIndex++;
        this.initChat();
        return this.sendMessage(message);
      }

      return 'ERRO_CRITICO: Instabilidade t√©cnica nos servi√ßos de IA.';
    }
  }

  reset() {
    this.currentModelIndex = 0;
    this.initChat();
  }
}

export const geminiService = new GeminiChatService();
